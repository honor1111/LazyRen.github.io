---
layout: post
title: "CS231N Lec. 10 | Recurrent Neural Networks"
subtitle: "cs231n lecture#9 review"
category: studies
tags: Lecture-Review
# image:
---

*please find lecture reference from here[^1]*


# CS231N Lec. 10 | Recurrent Neural Networks


# <b style='color:Skyblue'>*Recurrent Neural Networks*</b>.


![RNN](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-18-09.png)

We could get more flexibility from RNN. More options for in/output data types.

- One to many
  - image to sequenc
- Many to one
  - Setiment classification
  - Read Sentence sentiment
  - Comprehense Video contents (variable frames)
- Many to many
  - Machine Translation
  - English -> Korean
- Many to many
  - Video classification on frame level

But, RNN also useful fixed-size input like image. RNN could sequentially process non-sequential data. 
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-32-34.png)


So, the *__concept of RNN__* would be this.
![RNN-concept](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-33-46.png)

RNN formula.  
Note: The same function(f) and the same set of param. are used at every time step.
![RNN-formula](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-34-32.png)

*Vanlia RNN*  
Weight for prev. hidden layer(h_t-1) and input(x_t)
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-40-23.png)

<b style='color:Skyblue'>*RNN : Computational graph : Many to Many.*</b>
  

Unique h and x, but same Weight.

![Many2Many](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-43-49.png)

We can get loss per every time-step hidden layer. Total Loss would be sum of losses.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-46-22.png)

  
<b style='color:Skyblue'>*RNN : Computational graph : Many to One*</b>

e.g.) Sentiment 
![Many2One](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-49-55.png)


<b style='color:Skyblue'>*RNN : Computational graph : One to Many*</b>

e.g.) Sequential process of non-seq. data
![One2Many](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-50-23.png)


<b style='color:Skyblue'>*RNN : Computational graph : Many to One + One to Many*</b>


![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-20-52-41.png)

Example)

- Use input as each character for Vocab.
- Output is value of Softmax, to use probability distribution

By using probability distribution, we could get proper word "hello". If we use argmax, much easiler, but couldn't get proper answer.

![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-21-04-21.png)

*__Q. Why use sample, instead of just taking argmax?__*  
> *A. Good Question. As we can see above ex, if we take argmax, we couldn't reach out answer. But in practice, both are useful. Sometimes argmax maybe stable. But, samplingg gives you diversity of modle.*

But Problem is, too long test time for forward/backword through entire sequence to compute gradient. (image training entire Wekipedia) 
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-21-38-37.png)


One Solution is, <b style='color:Skyblue'>*Truncated Backprop.*</b>

It's *__approximation using minibatch__*, run forward/backword through chuncks of the sequence, instead of whole seq.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-21-43-28.png)

After 1st batch hidden layer, keep carrying them as forward. But backword, only use for some part of them.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-21-45-14.png)

This contiuned to the end.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-21-46-21.png)

*Q. Is this kind of "__Markovian assumption__[^2]" ?*  
> *A. Nope, hidden state is all to predict the entire future.*

Notable point is, it's not difficult, implemented just 112 lines of python, find [min-char-rnn.py](https://gist.github.com/karpathy/d4dee566867f8291f086) from gist.


Rnn is powerful for NLP.
After some train, model can say some nicer setences.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-22-22-15.png)

Even play.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-22-23-16.png)

Even Algebra topology....
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-22-24-03.png)


Back to the image processing,  
As we saw from *one to many + many to one*, we could apply CNN and RNN, makes model to say what image is.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-07-22-53-16.png)


But low performance when if non-trained input comes. It's not much easy to generalize.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-08-21-45-21.png)

## *Image caption with Attention*

- CNN output is a grid of vector 
- Get location to get attention(a1). Calculate with CNN out vector, and feed to next hidden layer.
- Two outputs comes out.
  - Distribution over vocab words.
  - Distribution over image location.(location to get attention)

![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-08-22-13-27.png)


## *Soft / Hard attention*

- *__Soft__*
  - Attention could be any of image location.
- *__Hard__*
  - Force model to look at one location of image.(related to reinforce learning) 

![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-08-22-18-16.png)


RNNs with Attention could be a good method for Visual Question Answering.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-08-22-23-36.png)

![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-08-22-26-24.png)

*Q. How to use two different inputs(question/image)*  
> *A. Usually concatenation used firstly and it's powerful. Other multiplicative interactions also used.*

<br>  

We've studied single-layer RNN, but usually multi-layer RNN used. But not deep, just 2~4 layers are typical.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-08-22-31-59.png)


## Gradient of Vanilla RNN

Let's consider gradient of vanilla RNN. As we saw before, gradient of matmul results into *multipling Transpose of Matrix*.
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-08-22-36-39.png)

If we consider multiple RNN layers, there should be multipling same W repeatedly ( because RNN have same W ).   
Let's assume W matrix as scalar and infinite layers to think easier. In this condition, 
- if W > 1, then exploded.
- if W < 1, then vanished.
only W = 1 would be only way to not happend aboves, but should be rare to happen. Same intuition applied to Matrix
- if Largest singular value > 1, then exploded.
  - Divide grad into grad_norm. (gradient clipping)
  - ![Gradient Clipping](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-13-22-13-07.png)
- if Largest singular value < 1, then vansihed. 
  - Change RNN architect
  - LSTM(Long Short Term Memory)

![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-08-22-39-26.png)

## LSTM (Long Short Term Memory)

[T-story(kor)](https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr) was helpful.


Many flow charts and diagrams are exist to explain LSTM. This time, Let's believe Standford. 
![LSTM](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-13-21-30-19.png)

- *f : Forget gate*  
  Whether to __erase__ cell
- *i : Input gate*  
  Whether to __write__ cell
- *g : Gate gate*  
  how much to __write__ to cell
- *o : Output gate*  
  how much to __reveal__ cell

<br>

*LSTM In backward pass*

As we know, in Vanila LSTM backprop., mutlply same W was main problem.

![LSTM-Backprop](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-13-21-42-00.png)


  
### Other RNNs
*GRU, LSTM : A Search Space Odyssey, ...* 
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-13-22-04-52.png)

<br>

## *GRU(Gated Recurrent Unit)*
GRU is simplied version of LSTM. 
![](/assets/img/posts/studies/lecture-review/lec10/md-img-paste-2020-12-13-22-42-30.png)

<br>

## Summary

- RNNs allow a lot of *__felxibility__* in architecture design.
- Vanilia RNNs are not that usefull.
- Commonly *__LSTM / GRU__* used : additive interactions improve gradient flow
- Backprop. of RNN can explode/vanish.
  - Explode : controlled with gradient clipping
  - Vanish : controlled with additive interaction(LSTM)
- There's a lot of nice overlap between CNN and RNN architecture.





***

[^1]: [Lecture(youtube)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) and [PDF](http://cs231n.stanford.edu/slides/)

[^2]: A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) *__depends only upon the present state, not on the sequence of events__* that preceded it. *from [Wiki](https://en.wikipedia.org/wiki/Markov_property)*