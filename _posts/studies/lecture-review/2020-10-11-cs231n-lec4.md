---
layout: post
title: "cs231n lecture#4 review"
subtitle: "cs231n lecture#4 review"
category: studies
tags: Lecture-Review
# image:
---

*please find lecture reference from here[^1]*

# CS231N Lec. 4 | Backpropagation and Neural Networks


### Recap gradient
- Numerical gradient
- Analytic gradient


### Computational graphs
linear classification과 같은 예시.
![computational graphs](/assets/img/posts/studies/lecture-review/lec4/markdown-img-paste-20201012185418363.png)


## *__Backpropagation__*



### Gradient
- Concept
node f 에 대해서 input(좌측)은 local gradient, output(우측)은 upstream gradient.
![gradient](/assets/img/posts/studies/lecture-review/lec4/markdown-img-paste-20201012214541993.png)
[local gradient] x [upstream gradient]  

실제로 활용 할 때 에는 이렇게 하나의 묶음으로써 자주 사용함.
그렇다면 *왜 굳이 하나씩 쪼개가면서 했는가?* 에 대한 [생각](https://en.wikipedia.org/wiki/Leaky_abstraction).
![sigmoid](/assets/img/posts/studies/lecture-review/lec4/markdown-img-paste-20201012223414805.png)

- gradient 각 연산의 의미
add : distributor
mul : switcher (scaler)
max : router
![gradient calculation](/assets/img/posts/studies/lecture-review/lec4/markdown-img-paste-20201012223256709.png)

- vectorized example  
![vectorized gradient](/assets/img/posts/studies/lecture-review/lec4/markdown-img-paste-2020101223023401.png)

gradient of L2 norm is just simply take derivate it.
![derivation of L2](/assets/img/posts/studies/lecture-review/lec4/markdown-img-paste-20201012230409494.png)

gradient of dot product is


신경망

activation function

[^1]: [Lecture(youtube)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) and [PDF](http://cs231n.stanford.edu/slides/)