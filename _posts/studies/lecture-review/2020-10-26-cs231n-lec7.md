---
layout: post
title: "CS231N Lec. 7 | Training Neural Networks II"
subtitle: "cs231n lecture#7 review"
category: studies
tags: Lecture-Review
# image:
---

*please find lecture reference from here[^1]*

# CS231N Lec. 7 | Training Neural Networks II


## Normalization 
Normalization make your model less sensity for variations.
![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-16-21-55-48.png)

<br>

## Problems with SGD 

1. *Sensitivity*  
if loss change one direction very quickly and slowly in another, (*in this case, quick for center, slow of circular*) then, like below, we might troubled in inefficient learning. This could be much worse for multiple dimension.

![SGD-prob1](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-16-21-56-59.png)

  
<br>

2. *Local Minima and saddle point*  
SGD can be stuck at local minima and saddle point(which is common for high-dimension).
![SGD-prob2](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-16-21-59-38.png)

<br>

e.g) one example for poor movement of SGD 

![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-16-22-06-28.png)

<br>

## Problem solving - Add Momentum

- Add tiny "momentum" to maintain movement.

![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-16-22-08-47.png)

Idea is quite simple, but result is very strong

![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-16-22-13-08.png)

<br>

### Nesterov Momentum

Kind of error fixing for previous velocity.

![Nesterov](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-21-16-12.png)


*Why we learn annoying thing? because it's effectvie...*

![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-21-17-32.png)

__Q__. What if velocity is fast so, passing sharp minima?  
__A__. Good question, actually we ignore that kind of sharp minima. Because, it's kind of overfitting. So, If environment changed, sharp minima could be disappeared. In some sense, SGD+Momentum have a feature ignoring sharp minima. 

<br>

### __*AdaGrad*__

- Adding square of grad & divide root of this

This make slow make faster, fast make slower.

![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-21-36-19.png)

- Cons
But if learning progress so far, step size becomes too small.

<br>

### __*RMSProp*__

- Add decay rate for AdaGrad.

![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-21-53-40.png)

<br>

### __*Adam (draft)*__ 

- Mix of Adagrad + RMSProp
![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-21-51-22.png)

- Problem
We've initialized "_second_moment_" as 0, so at the 1st step "second_moment" should be very small, we divide by "_second_moment_" which is very small number, resulting x changed very large.

<br>

### __*Adam (full form)*__

Adam full form fixed above problem of Adam (draft), by bias correction.
Adam shows quite good performance for multiple models. Usually used for default algorithm.   
Try use **beta1 = 0.9** / **beta2 = 0.999** / **learning_rate = 1e-3 or 1e-5**

![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-22-50-18.png)



_Q. What problem Adam still have?_  
_A. If Oval tilted, than problem we observed at SGD, zigzaging, will be reproduced._

<br>

## *__Learning rate decay__*

Exponential and 1/t decay exist. 
It's common for SGD, __less__ common with Adam.  
Also, it's secondary hyperparameter. you shouldn't fit learning rate decay at first.

![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-23-05-00.png)

<br>

## First-Order Optimization

We've talked about this algorithm.
![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-23-10-36.png)


## Second-Order Optimization

Little bit fancier.
![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-23-11-21.png)

*Q1. What is nice?*  
*A2. It doesn't have *__Learning rate__* !.*  
Because we could just calculate both dots. ( But only for vanila version )
![](/assets/img/posts/studies/lecture-review/lec7/md-img-paste-2020-11-18-23-11-58.png)

*Q2. Why is this bad for deep learning?*  
*A2. Hessian has __O(N^2)__ elements, inverting takes __O(N^3)__ which is quite expensive. So, in practical, people use approximation.*

<br>

_**In practice**_ :

- __Adam__ for good default option. 
- If full batch affordable, try out __L-BFGS__



[^1]: [Lecture(youtube)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) and [PDF](http://cs231n.stanford.edu/slides/)