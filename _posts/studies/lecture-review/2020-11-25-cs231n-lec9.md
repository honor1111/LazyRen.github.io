---
layout: post
title: "CS231N Lec. 9 | CNN Architectures"
subtitle: "cs231n lecture#9 review"
category: studies
tags: Lecture-Review
# image:
---

*please find lecture reference from here[^1]*


# CS231N Lec. 9 | CNN Architectures

# CNN Architecture

Today lecture is about CNN Architectures.
![CNN-Archis](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-20-01-05.png)


<h2><b><i><span style="color:skyblue">AlexNet</span></h3>  


 
Let's start with AlexNet ( *with quiz* )

![](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-21-04-27.png)

Q1. 
![AlexNet-Q1](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-21-06-22.png)

Answer is, __55 x 55 x 96__ (W_out x H_out x Num-of-filters)

Q2.
![AlexNet-Q2](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-21-07-53.png)

Answer is, __(11x11x3)x96__ (W-filter x H-filter x input-depth)xNum-of-filters

Q3.
![AlexNet-Q3](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-21-12-05.png)

Answer is, __27x27x96__ (W_out x H_out x Num-of-depth)

Q4.
![AlexNet-Q4](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-21-15-06.png)

Answer is, 0 !! (Because it's *__pooling layer__*, nothing to learn. )

<br>

*Historical Note*  
At 2012, traine was done at GTX580(*quite old GPU*), which only have 3GB memory.
so divide neuron into half, and learned at each GPU.
![AlexNet-HistNote](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-21-57-34.png)

<br>

*Achievement*  
AlexNet is 1st winner of CNN-based network at 2012.

![AlexNet-ILSVRC](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-22-02-42.png)

<br>

<h2><b><i><span style="color:skyblue">GoogleNet and VGGNet</span></h3>  

Deeper! Better! ( than AlexNet )

![](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-22-07-59.png)

<br>

<h3><b><i><span style="color:skyblue">VGGNet</span></h3>  

Smaller filters, Deeper networks.

![VGGNet](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-22-09-11.png)

Q1.
![VGG-Q1](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-22-11-58.png)

A1.  
With smaller filter, we could have deeper network.
__three (3x3) conv (stride 1)__ layers has same effective receptive field as __one (7x7) conv layer__.

Q1-1.
![VGG-Q1-1](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-22-21-49.png)

A1-1.  
Answer is [7x7].  
At every coners of [3x3] looks another [3x3], so, becomes 3->5->7.
![VGG-A1-1](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-22-21-36.png)

So, by using smaller filter, we can get *deeper* and more *non-linearities*.  
And, fewer params : 3*(3^2C^2) vs. 7^2C^2  
(*C : input depth, total number of output feature map*) 

So the result would be like below:
![VGG](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-25-23-57-18.png)

*Q. For each layer, what do different filters need?*  
*A. Each layer means, one feature map, one activation map of all the responses of different spatial location. Each filter correspond to a different pattern that we looking for in the input.*

*Q. Intuition for deeper network, more channel depth, more number of filters ?*  
*A. Deeper network is not mandatory. One reason is, relatively constant level of compute. Another reason is, when you go deeper, usually you'll use down sampling, which is less expensive to make deeper network.*  

*Q. I don't think we should memorize all of the data, we could throw away some parts.*  
*A. Yes, some are we don't need to keep. But we also need to save for further backprop.. So the large part of them need to save.*   

<h3><span style="color:skyblue">GoogLeNet</span></h3>  
*Deeper networks, with computational efficiency.*

Characteristic point of GoogLeNet is, network of network(Inception Model).
![GoogLeNet](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-20-46-26.png)

Appling parallel filter operation on the input from previous layer.
- Multiple receptive field sizes for conv. (1x1, 3x3, 5x5)
- Pooling (3x3)  

And then, concate all filter outputs together depth-wise.

Q. What is the problem with this ?  
A. Computational complexity.

e.g.) let's see *__naive__* implementation of googlenet.  
By Zero padding, we could get all same Width & Height with different depth. And at concat. filter, adding all depth. 
![GoogLeNet-EX](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-21-12-07.png)

Quite expensive cost !
![GoogLeNet-Cost](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-21-19-54.png)

Solution : bottleneck !  
With 1x1 CONV, we can preserve spatial demensions, with reduced depth.

So, the GoogLeNet would be below: 
![GoogLeNet-comparision](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-21-24-09.png)

With 1x1 CONV (bottleneck)
- Operation reduced 854M --> 358M (-58%)
- Depth reduced 672 --> 480 (-29%)
![GoogLeNet-comparision2](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-21-24-52.png)

Full Version of GoogLeNet.
![GoogLeNet-FullVer](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-21-46-55.png)

*Q. Does auxiliary layer actually make better results?*
*A. Yes, but you'll need to check details.*

*Q. Can i use non-1x1 CONV at bottleneck?*
*A. Yes, but benefits by reducing will be decreased.*

*Q. All of layers are sepearted W?*
*A. Yes.*

*Q. Why adding auxiliary outputs to inject gradient?*  
*A. When layers going deeper, some signals can be vanished. This can be helped by provide additional signal(auxiliary outputs)*

<h3><span style="color:skyblue">ResNet</span></h3>  
Revolutionary deep networks using ***residual connection***.

![ResNet](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-21-52-17.png)

Let's compare 'plain' deep vs shallow CNN.  
As you can see at below graph, 56-layer performance worse. ( But it also worse for Training, so no matter of overfitting. ) 
![Deep-vs-shallow Network](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-22-19-15.png)

Following hypothesis is, the problem is an *optimization* problem.  Deeper models are harder to optimize(cuz have more param.)

> A solution by construction is copying the learned
layers from the shallower model and setting
additional layers to identity mapping.

![ResNet-concept](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-22-47-16.png)

So the concept is, generally, Initial(shallow) learned layer would be good. 
- *Make Initial-learned-layer. (should fine and not deep)*
- *Make deeper layers by identity mapping, which would maintain Initial-learned-layer. (because hypothesis implies good-final-layer should not much different with Initial-learned-layer)*
- *Make model learn the identity. (if init-learned-model is best, F(x) would be zero, and seems much easier to learning. Because learning to not different with identity.)*
- *Finally get better score of Initial-learned-layer.*

Therefore, i could summarize ResNet as 
1. *Make good "Initial-learned-layer".* 
2. *Go deeper, but try to learn "Initial-learned-layer".*

*!! But mind that aboves are just hypothesis of ResNet !!*

ResNet used "BottleNeck" in case of deeper network more than ResNet-50.
![](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-23-58-48.png)


![ResNet-Practice](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-11-30-23-57-46.png)


<h2><span style="color:skyblue">Other architectures</span></h3>  

### Network in Network (NiN)

- precursor of bottleneck of GoogLeNet
![NiN](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-00-13-15.png)


### Wide Residual Networks

![Wide-ResNet](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-00-17-38.png)

### ResNext

![ResNext](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-00-18-47.png)

### Deep Networks with Stochastic Depth

![DeepNetWithStochasticDepth](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-00-21-19.png)


### FractalNet : Ultra-deep Neural Networks w/o Residuals

Argues that key is *transitioning effectively from shallow to deep*
and residual representations are not necessary.
![FractalNet](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-20-47-15.png)


### DensNet

![DenseNet](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-20-45-42.png)

### SqueezeNet

![SqueezeNet](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-20-49-51.png)


<h2><span style="color:skyblue">Summary</span></h3>  



![Summary](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-20-52-04.png)

![Summary2](/assets/img/posts/studies/lecture-review/lec9/md-img-paste-2020-12-01-20-53-11.png)

***

[^1]: [Lecture(youtube)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) and [PDF](http://cs231n.stanford.edu/slides/)