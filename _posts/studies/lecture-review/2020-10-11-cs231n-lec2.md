---
layout: post
title: "cs231n lecture#2 review"
subtitle: "cs231n lecture#2 review"
category: studies
tags: Lecture-Review
# image: 
---

*please find lecture reference from here[^1]*

## KNN 
*K-nearest neighbor*
L1 ( *Manhattan* ) distance
dependency on "coordinate system"

L2 ( *Euclidean* ) distance

>   *which one is better?*    it's hyperparameter, so it's better to try both.

__*Hyperparameter*__  
>   Choices about the algorithm that we __set__ rather than learn.  
    Quite problem dependent. should try all possible cases.  
    e.g.) in case of KNN, _value of k_ and _distance_.

### How set Hyperparameter ?
1. __split data__  
When you Setting Hyperparameter, split data into ***train***, ***validation*** and ***test***  
choose hyperparameters on validation and evaluate on test.  
![split data](\assets\img\posts\studies\lecture-review\2020-10-11-cs231n-lec2_distance.png)
  
2. __cross validation__  
split data into __folds__, try each fold as validation and average the result.
![cv](\assets\img\posts\studies\lecture-review\2020-10-11-cs231n-lec2_cv.png)

useful for *__small datasets__*. not commonly used at ~~deep learning~~.

As you can see above demo, KNN seems not good algorithm for groupping.
And actually it does. KNN is not good method for Computer Vision.







[^1]: [Lecture(youtube)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) and [PDF](http://cs231n.stanford.edu/slides/)