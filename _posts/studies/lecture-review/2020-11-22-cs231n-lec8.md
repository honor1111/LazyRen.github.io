---
layout: post
title: "cs231n lecture#8 review"
subtitle: "cs231n lecture#8 review"
category: studies
tags: Lecture-Review
# image:
---

*please find lecture reference from here[^1]*

# CS231N Lec. 8 | Deep Learning Software


# DeepLearning Software

There are some useful S/W which make us much easer and faster when implementing deep learning.

<br>

## CPU vs GPU for Deep Learning

During deep learning, many matrix multiplication occurrs. ( e.g. Weight x Input X )  
To handle this mat mul much faster, using GPU can be a good option. 

Base on below CPU vs GPU comparison, we can find CPU have higher Clock speed without RAM, while GPU have many cores and memory.  
This means, *__GPU is good for simple-parallel tasks__*.
![CPUvsGPU](/assets/img/posts/studies/lecture-review/lec8/md-img-paste-2020-11-22-11-09-42.png)


In practice, we can observe more than x60 effectiveness.  
( *But should aware that it's unfair comparison. These deeplearning algorithms are optimized for GPU* )
![](/assets/img/posts/studies/lecture-review/lec8/md-img-paste-2020-11-22-11-05-31.png)

And, cuDNN (*which is optimized code for [CUDA](https://en.wikipedia.org/wiki/CUDA)[^2]*) can work much faster then raw CUDA code.
![](/assets/img/posts/studies/lecture-review/lec8/md-img-paste-2020-11-22-11-15-41.png)

Actually in practice, below frameworks already use cuDNN for mutliple cases, so in usual case, we barely need to take a look into cuDNN. ( *Lecturer also didn't* )
![2017-Framework](/assets/img/posts/studies/lecture-review/lec8/md-img-paste-2020-11-22-11-22-33.png)


Tensorflow is similar to Numpy, while PyTorch is less verbose.
![np-tf-pytorch](/assets/img/posts/studies/lecture-review/lec8/md-img-paste-2020-11-22-11-29-04.png)

These framework activley changed. There are no answer for every tasks.  
Like, tensor started faster then others so it have many users. But less effective.  
While newer frameworks seems to be much effective, but less users.

<br>

## Tensorflow

We can make tensorflow code as below.
![](/assets/img/posts/studies/lecture-review/lec8/md-img-paste-2020-11-22-11-48-22.png)

And could train. But we keep copy new value in "values".  
Meaning, new value calculated from tensor, copied into numpy array "values".  
That is, we used GPU(tensorflow) - CPU(numpy) both. This could be problem in huge model.
![naive-tensor](/assets/img/posts/studies/lecture-review/lec8/md-img-paste-2020-11-22-11-49-18.png)








[^1]: [Lecture(youtube)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) and [PDF](http://cs231n.stanford.edu/slides/)
[^2]: CUDA
