---
layout: post
title: "cs231n lecture#6 review"
subtitle: "cs231n lecture#6 review"
category: studies
tags: Lecture-Review
# image:
---


# Training Neural Networks I


*Let's begin to learn ..*

>    1.  __One time setup__  
        *activation functions, preprocessing, weight  
        initializing, regularization, gradient checking*
>    2.  __Training dynamics__  
        *babysitting the learning process,  
        parameter updates, hyperparameter optimization*
>    3.  __Evaluation__  
       *model ensembles*

<br>

## __Part 1__

- [Activation functions](#Activation-functions)
- [Data Preprocessing](#Data-Preprocessing)
- [Weight Initialization](#Weight-Initialization)
- [Batch Normalization](#Batch-Normalization)
- [Babysitting the Learning Process](#Babysitting-the-Learning-Process)
- [Hyperparameter Optimization](#Hyperparameter-Optimization)

<br>
   
### __1. Activation functions__<a id="Activation-functions"></a>  

at any of certain layer, inputs comes in and multiplied by some weight(w_i, plus bias)  
and then activate it by *__some of functions__*
![](assets/md-img-paste-2020-10-24-16-30-25.png)

the functions are.. 
![](assets/md-img-paste-2020-10-24-16-36-07.png)


### Sigmoid

Output always returns [0,1].  
Historically popular, since it have nice performance of saturating value.  
But have this have 3 known <b><i><span style="color:RED">Problems</span>

![a](assets/md-img-paste-2020-10-24-16-52-35.png)

<b><i><span style="color:RED">Problem 1</span>  

When x = -10, gradient would be __0__  
When x = 0, gradient would be __fine(proper value for neural network..)__  
When x = 10, gradient would be __0__  

![](assets/md-img-paste-2020-10-24-17-10-35.png)


<b><i><span style="color:RED">Problem 2</span>  

asd
![](assets/md-img-paste-2020-10-24-17-26-19.png)

<b><i><span style="color:RED">Problem 3</span>  

asd
