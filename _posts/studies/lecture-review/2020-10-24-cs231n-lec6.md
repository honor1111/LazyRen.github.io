---
layout: post
title: "CS231N Lec. 6 | Training Neural Networks I"
subtitle: "cs231n lecture#6 review"
category: studies
tags: Lecture-Review
# image:
---


*please find lecture reference from here[^1]*

# CS231N Lec. 6 | Training Neural Networks I

*Let's begin to learn ..*

>    1.  __One time setup__  
        *activation functions, preprocessing, weight  
        initializing, regularization, gradient checking*
>    2.  __Training dynamics__  
        *babysitting the learning process,  
        parameter updates, hyperparameter optimization*
>    3.  __Evaluation__  
       *model ensembles*

<br>

## __Part 1__

- [Activation functions](#Activation-functions)
- [Data Preprocessing](#Data-Preprocessing)
- [Weight Initialization](#Weight-Initialization)
- [Batch Normalization](#Batch-Normalization)
- [Babysitting the Learning Process](#Babysitting-the-Learning-Process)
- [Hyperparameter Optimization](#Hyperparameter-Optimization)

<br>
   
---

<br>

### __1. Activation functions__<a id="Activation-functions"></a>  

at any of certain layer, inputs comes in and multiplied by some weight(w_i, plus bias)  
and then activate it by *__some of functions__*
![](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-16-30-25.png)

the functions are.. 
![](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-16-36-07.png)


<br>


### _Sigmoid_

Output always returns [0,1].  
Historically popular, since it have nice performance of saturating value.  
But have this have 3 known <b><i><span style="color:RED">Problems</span>

![a](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-16-52-35.png)

<b><i><span style="color:RED">Problem 1 - gradient loss</span>  

When x = -10, gradient would be __0__  
When x = 0, gradient would be __fine(proper value for neural network..)__  
When x = 10, gradient would be __0__  

So, when activation function is "Sigmoid", then gradient can be lost.

![problem1](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-17-10-35.png)


<b><i><span style="color:RED">Problem 2 - output is not zero-centered</span>  

As above graph, Sigmoid is not zero-centered.  
This will cause quite inefficiency.   

![problem2-1](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-18-54-48.png)

Let's think of gradient of w.  
derivation of sigmoid f is df/dw = x, that is, dL/dw =  dL/df(upstream gradient) * df/dw(==x, local gradient).  

at this, we assume all x is positive. so, the sign of dL/dw is dependent to dL/df.  
it means, sign of gradient is all positive or negative depends on sign of dL/df(upstream gradient).

at below graph, <b><i><span style="color:blue">optimal gradient</span> of w is straight forward. But, because of <b><i><span style="color:red">problem2</span>(_not zero centered_), gradient is going as <b><i><span style="color:red">zig zang path</span> as below.

So, sigmoid could cause *__inefficiency__* problem. 

![problem2-2](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-18-33-44.png)

<b><i><span style="color:RED">Problem 3</span>  

It's quite not main problem, in the grand scheme of network.( conv. and dot-product is dominant )  
But it is true that exponential is have computational expensive. ( think of P-NP )



<br>


### _tanh_

It's bit better than sigmoid.   
As you can see below figure, this *__solve zero-centered problem__* of sigmoid.  
But this *__still kills gradient__* when saturated.

![tanh](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-19-08-57.png)


<br>


### _ReLU_ (*Rectified Linear Unit*)

This solve many problems observed from sigmoid/tanh.
- Not saturated ( in positive region )
- Much computational efficient. ( linear )
- Converges much faster than sigmoid/tanh in practice ( e.g. 6x )
- More biologically plausible than sigmoid/tanh ( cuz we're mimicking neuron )

Alexnet, which used ReLU, is major CNN algorithm which could do well on ImageNet and larget-scale data.

![ReLu](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-19-19-32.png)

<b><i><span style="color:orangered">But this still have 2 problems</span>, 
- Not zero-centered
- An annoyance ( still saturated at negative region)

So, with below two cases, "Dead ReLU" can be increased.
- bad initialization
- learning rate is too high
![Dead-ReLu](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-19-30-53.png)


People like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)
Not all use, but people use to increase likihood of it being active on beginning.



### _Leaky ReLU_

With simple revision, saturation problem of ReLU can be solved by below two ReLU.

![Leaky and Parametric ReLU](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-19-41-39.png)


### ELU

Another revision of ReLU. This use exponential at negative regime.  
So, This could adds some robustness to noise. But Computation requires exp.

![ELU](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-19-44-12.png)

### _Maxout "Neuron"_

Not saturated. Abandone linearity and get generalized ReLU and Leaky ReLU.  
But <b><i><span style="color:orangered">doubles # of param./neuron.</span>


![Maxout Neuron](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-19-51-25.png)

TLDR (Too Long Didn't Read).. In practice, 

- Use <b><i><span style="color:green">ReLU</span>(*but be careful with your learning rate*)  
- Try <b><i><span style="color:olive">Leaky ReLU / Maxout / ELU</span>  
- Try <b><i><span style="color:orangered">tanh</span>, but don't expect much
- <b><i><span style="color:red">DON'T USE sigmoid</span>

<br>

--- 

<br>

### __2. Data Preprocessing__<a id="Data-Preprocessing"></a>  


### Preprocessing the data  
  
With below preprocessing(zero-centered, normalize, PCA, Whitening), It's good to get better performance.  
But typically, only <b><i><span style="color:green">zero-centered</span> used, to get general performance for images. 

Actually, images are already *__normalized__* by RGB(0~255). So, don't doing additional normalization.

![preprocessing-1](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-20-16-39.png)
![preprocessing-2](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-20-16-20.png)


Q. Do preprocessing is started once we start learning, not doing per batch?  
A. Yes. After enough good batches, then could get good estimate of mean. (even though not training entire data)

Q. Does this zero-centered pre-processing solve sigmoid problem from not-zero-centered?  
A. Nope, just only first layer only solved. Not sufficient to further deep learning.

<br>

### __3. Weight Initialization__<a id="Weight-Initialization"></a>  

Let's assume all W init. as 0.  
All neurons will doing same thing and update as samely. Which is what we don't want.
( each neurons are different based on which class it was connected to, but when looking in all neural network, it going to be same.)

1. Small random number

Good for only small. But for deeper case, all activations become zero.
![W-init-small-random](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-20-55-53.png)
![W-init-small-random2](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-00-19.png)

in case of 100times bigger then 0.01, then becomes saturated either 1 and -1
![W-init-small-random3](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-03-59.png)

_Xaiver Initialization_

Good for linear case.
![Xavier-Init](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-06-47.png)

But bad for nonlinear case. (ReLU)
![Xavier-Init2](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-07-43.png)

But! can be improved by *__additional /2__*
![Xavier-Init3](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-08-29.png)

Many researchers are studying for good initializing. Plz refer to belows..  

>
    - Understanding the difficulty of training deep feedforward neural networks by Glorot and Bengio, 2010  
    - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe et al, - 2013  
    - Random walk initialization for training very deep feedforward networks by Sussillo and Abbott, 2014  
    - Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification by He et al., 2015  
    - Data-dependent Initializations of Convolutional Neural Networks by Krähenbühl et al., 2015  
    - All you need is a good init, Mishkin and Matas, 2015  

A good general rule of thumb is basically use the Xaiver Initialization to start with, and think of others.


<br>

### __4. Batch Normalization__<a id="Batch-Normalization"></a>  
*Recommend to read this paper*  

BN is a idea to preventing Gradient vanashing.  
Training samples : N
Dimension : D
![Batch-Norm-1](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-16-47.png)

Usually inserted after Fully connected or Convolutional layers and before nonlinearity.
![Batch-Norm-2](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-21-03.png)

Q. Do we necessarily want a unit gaussian input to a tanh layer?
A. In point of controlling "how to saturated", yes. After normalized, network can squash the range by learning.  
Both gamma and beta is param. to be learned, gamma is scaling and beta is shifting. These are used for giving flexibility to how BN works. In tanh example, by learning network can choose how to be saturated.
![Batch-Norm-3](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-36-29.png)

Summary of BN.

- Good gradient flow
- Allow higher learning rate
- Reduce dependency of initialization
- Slightly reduces the needs for dropout.

![Batch-Norm-4](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-21-40-32.png)


Q. Do BN could good for small batch size case like reinforcement?  
A. Usually no, but have same effects, commonly good.

Q. Do BN change structure?  
A. No, it just scaling and shifting.

Q. Does BN could be redundant by recovering identify mapping ?  
A. In practice, no. Not learning same as identify mapping.

Q. Does BN make gaussian ?  
A. No, it's approximate values.


Batch Norm. is computed at training time(*like running average*) and used at testing time.  

![Batch-Norm-5](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-18-28.png)

<br>

### __5. Babysitting the Learning Process__<a id="Babysitting-the-Learning-Process"></a>  

Double check that the loss is reasonable.

![Babysitting-1](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-24-05.png)
sanity check, checking model works as when designed.
![Babysitting-2](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-24-22.png)


Trial for small samples to overfit. 
![Babysitting-3](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-26-51.png)
![Babysitting-4](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-27-46.png)


Let's begin with regularization.   
Loss barely changing, cuz learning rate is probably too low.  
But it is training.
![Babysitting-5](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-29-52.png)

Let's change learning rate bigger. 1e6.. super big..  
returns NaN, meaning cost exploded.
![Babysitting-6](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-35-56.png)

Rough ranger for learning is [1e-3, 1e-5].
![Babysitting-7](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-37-26.png)

<br>

### __6. Hyperparameter Optimization__<a id="Hyperparameter-Optimization"></a>  
How to pick best hyperparameters?  
: Try cross-validation

- with few epoches to get rough idea.
- longer running time, finer search.  
... repeat

if cost is over > 3*original cost, then break out.

Example for searching hyperparam. it's good to use log scale value.
![Hyperparameter-1](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-44-01.png)


Adjusting and observe reulsts...
All learning rate is saturated at e-04, which means better lr could be exist less then e-04.
![Hyperparameter-2](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-48-20.png)


How to search..
![Hyperparameter-3](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-52-25.png)

Learning rate samples
![Hyperparameter-4](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-55-46.png)

Interpreting Loss graph
![Hyperparameter-5](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-56-13.png)

Monitoring accuracy.
![Hyperparameter-6](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-56-49.png)

Tracking W diff.
![Hyperparameter-7](/assets/img/posts/studies/lecture-review/lec6/md-img-paste-2020-10-24-22-57-13.png)

TLDR : Summary

- Activation Functions (*use ReLU*)
- Data Preprocessing (*images: subtract mean*)
- Weight Initialization (*use Xavier init*)
- Batch Normalization (*use*)
- Babysitting the Learning process
- Hyperparameter Optimization (*random sample hyperparams, in log space when appropriate*)


[^1]: [Lecture(youtube)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) and [PDF](http://cs231n.stanford.edu/slides/)